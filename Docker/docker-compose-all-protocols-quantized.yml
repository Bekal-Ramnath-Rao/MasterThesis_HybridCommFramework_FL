version: '3.8'

# Multi-Protocol Federated Learning with Quantization
# Demonstrates all 5 protocols with quantization enabled
# Use Case: Emotion Recognition
services:
  # ============================================================================
  # MQTT PROTOCOL with Quantization
  mqtt-broker:
    image: eclipse-mosquitto:2
    container_name: fl-mqtt-broker
    ports:
      - "1883:1883"
    volumes:
      - ../mqtt-config:/mosquitto/config
    networks:
      - fl-network
    command: mosquitto -c /mosquitto/config/mosquitto.conf
  fl-server-mqtt:
    build:
      context: ..
      dockerfile: Server/Dockerfile
    container_name: fl-server-mqtt-quantized
    depends_on:
      - mqtt-broker
      - "8081:8080"
    environment:
      - USE_QUANTIZATION=true
      - QUANTIZATION_BITS=8
      - MQTT_BROKER=mqtt-broker
      - NUM_CLIENTS=2
      - NUM_ROUNDS=3
      - MIN_CLIENTS=${MIN_CLIENTS:-2}
      - MAX_CLIENTS=${MAX_CLIENTS:-100}
    command: python -u Server/Emotion_Recognition/FL_Server_MQTT.py
  fl-client-mqtt-1:
      dockerfile: Client/Dockerfile
    container_name: fl-client-mqtt-1-quantized
      - fl-server-mqtt
      - CLIENT_ID=1
    command: python -u Client/Emotion_Recognition/FL_Client_MQTT.py
      - ../Client/Emotion_Recognition/Dataset:/app/Client/Emotion_Recognition/Dataset
  fl-client-mqtt-2:
    container_name: fl-client-mqtt-2-quantized
      - CLIENT_ID=2
  # gRPC PROTOCOL with Quantization
  fl-server-grpc:
    container_name: fl-server-grpc-quantized
      - "50051:50051"
      - QUANTIZATION_BITS=16              # 2x compression for gRPC
      - GRPC_PORT=50051
    command: python -u Server/Emotion_Recognition/FL_Server_gRPC.py
  fl-client-grpc-1:
    container_name: fl-client-grpc-1-quantized
      - fl-server-grpc
      - GRPC_HOST=fl-server-grpc
      - CLIENT_ID=0
    command: python -u Client/Emotion_Recognition/FL_Client_gRPC.py
  fl-client-grpc-2:
    container_name: fl-client-grpc-2-quantized
      - QUANTIZATION_BITS=16
  # AMQP PROTOCOL with Quantization
  rabbitmq:
    image: rabbitmq:3-management
    container_name: fl-rabbitmq
      - "5672:5672"
      - "15672:15672"
  fl-server-amqp:
    container_name: fl-server-amqp-quantized
      - rabbitmq
      - AMQP_HOST=rabbitmq
    command: python -u Server/Emotion_Recognition/FL_Server_AMQP.py
  fl-client-amqp-1:
    container_name: fl-client-amqp-1-quantized
      - fl-server-amqp
    command: python -u Client/Emotion_Recognition/FL_Client_AMQP.py
  fl-client-amqp-2:
    container_name: fl-client-amqp-2-quantized
networks:
  fl-network:
    driver: bridge
# ============================================================================
# USAGE:
# Run specific protocol:
#   docker-compose -f docker-compose-all-protocols-quantized.yml up mqtt-broker fl-server-mqtt fl-client-mqtt-1 fl-client-mqtt-2
#   docker-compose -f docker-compose-all-protocols-quantized.yml up fl-server-grpc fl-client-grpc-1 fl-client-grpc-2
#   docker-compose -f docker-compose-all-protocols-quantized.yml up rabbitmq fl-server-amqp fl-client-amqp-1 fl-client-amqp-2
#
# Run all protocols (heavy resource usage):
#   docker-compose -f docker-compose-all-protocols-quantized.yml up
# View logs for specific protocol:
#   docker logs fl-server-mqtt-quantized
#   docker logs fl-server-grpc-quantized
#   docker logs fl-server-amqp-quantized
# Quantization verification - look for:
#   "Server: Quantization enabled"
#   "Client X: Quantization enabled"
#   "Compressed weights - Ratio: 4.00x" (for 8-bit)
#   "Compressed weights - Ratio: 2.00x" (for 16-bit)
